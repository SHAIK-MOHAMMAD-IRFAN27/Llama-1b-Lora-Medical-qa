{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:47:18.530224Z",
     "iopub.status.busy": "2025-06-14T10:47:18.530035Z",
     "iopub.status.idle": "2025-06-14T10:47:24.236881Z",
     "shell.execute_reply": "2025-06-14T10:47:24.235929Z",
     "shell.execute_reply.started": "2025-06-14T10:47:18.530207Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: peft 0.14.0\n",
      "Uninstalling peft-0.14.0:\n",
      "  Successfully uninstalled peft-0.14.0\n",
      "Found existing installation: transformers 4.51.3\n",
      "Uninstalling transformers-4.51.3:\n",
      "  Successfully uninstalled transformers-4.51.3\n",
      "Found existing installation: accelerate 1.5.2\n",
      "Uninstalling accelerate-1.5.2:\n",
      "  Successfully uninstalled accelerate-1.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y peft transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:47:24.238777Z",
     "iopub.status.busy": "2025-06-14T10:47:24.238444Z",
     "iopub.status.idle": "2025-06-14T10:48:53.032527Z",
     "shell.execute_reply": "2025-06-14T10:48:53.031825Z",
     "shell.execute_reply.started": "2025-06-14T10:47:24.238750Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers peft bitsandbytes accelerate trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:48:53.033909Z",
     "iopub.status.busy": "2025-06-14T10:48:53.033593Z",
     "iopub.status.idle": "2025-06-14T10:49:18.328115Z",
     "shell.execute_reply": "2025-06-14T10:49:18.327295Z",
     "shell.execute_reply.started": "2025-06-14T10:48:53.033877Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 10:49:05.061780: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749898145.266547      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749898145.326780      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 BIT QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:51:26.430422Z",
     "iopub.status.busy": "2025-06-14T10:51:26.430070Z",
     "iopub.status.idle": "2025-06-14T10:51:26.436372Z",
     "shell.execute_reply": "2025-06-14T10:51:26.435650Z",
     "shell.execute_reply.started": "2025-06-14T10:51:26.430399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:51:26.824858Z",
     "iopub.status.busy": "2025-06-14T10:51:26.824536Z",
     "iopub.status.idle": "2025-06-14T10:51:26.828759Z",
     "shell.execute_reply": "2025-06-14T10:51:26.827942Z",
     "shell.execute_reply.started": "2025-06-14T10:51:26.824836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HUGGINGFACE SECRET TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T19:08:27.493233Z",
     "iopub.status.busy": "2025-06-14T19:08:27.492652Z",
     "iopub.status.idle": "2025-06-14T19:08:27.644571Z",
     "shell.execute_reply": "2025-06-14T19:08:27.643826Z",
     "shell.execute_reply.started": "2025-06-14T19:08:27.493210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:51:27.559716Z",
     "iopub.status.busy": "2025-06-14T10:51:27.559323Z",
     "iopub.status.idle": "2025-06-14T10:51:27.669447Z",
     "shell.execute_reply": "2025-06-14T10:51:27.668892Z",
     "shell.execute_reply.started": "2025-06-14T10:51:27.559681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 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\n",
    "from huggingface_hub import login\n",
    "login(secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:51:29.560720Z",
     "iopub.status.busy": "2025-06-14T10:51:29.560375Z",
     "iopub.status.idle": "2025-06-14T10:51:31.452933Z",
     "shell.execute_reply": "2025-06-14T10:51:31.452255Z",
     "shell.execute_reply.started": "2025-06-14T10:51:29.560696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for meta-llama/Llama-3.2-1B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee2f6dc241f4601b54e7a802638b384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bb55a4056240f7aeaa7105fc3bc5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e3cca5a22843bb85db08ef9df434dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Loading tokenizer for {model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Essential for consistent padding during training\n",
    "tokenizer.padding_side = \"right\" # Helps with causal language modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:51:31.454420Z",
     "iopub.status.busy": "2025-06-14T10:51:31.454135Z",
     "iopub.status.idle": "2025-06-14T10:51:46.468121Z",
     "shell.execute_reply": "2025-06-14T10:51:46.467255Z",
     "shell.execute_reply.started": "2025-06-14T10:51:31.454396Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model meta-llama/Llama-3.2-1B in 4-bit precision...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4822a5e99cc445689a261e3a48316dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9888469dc18e4129b4ab9207c6ce4e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f974903ba54737a6ef502eeb9e7a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Loading model {model_id} in 4-bit precision...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}, \n",
    "    torch_dtype=torch.bfloat16, # This sets the default compute dtype\n",
    "    trust_remote_code=True, # May be needed for some newer models\n",
    ")\n",
    "\n",
    "# Disable cache to save memory during training and ensure reproducibility\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:51:46.470000Z",
     "iopub.status.busy": "2025-06-14T10:51:46.469526Z",
     "iopub.status.idle": "2025-06-14T10:51:46.503462Z",
     "shell.execute_reply": "2025-06-14T10:51:46.502568Z",
     "shell.execute_reply.started": "2025-06-14T10:51:46.469970Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for k-bit training (QLoRA)...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing model for k-bit training (QLoRA)...\")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:51:46.504927Z",
     "iopub.status.busy": "2025-06-14T10:51:46.504481Z",
     "iopub.status.idle": "2025-06-14T10:51:55.027653Z",
     "shell.execute_reply": "2025-06-14T10:51:55.026837Z",
     "shell.execute_reply.started": "2025-06-14T10:51:46.504901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching LoRA adapters to the model...\n",
      "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\", \n",
    "    ]\n",
    ")\n",
    "print(\"Attaching LoRA adapters to the model...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:51:55.029449Z",
     "iopub.status.busy": "2025-06-14T10:51:55.029152Z",
     "iopub.status.idle": "2025-06-14T10:51:59.886251Z",
     "shell.execute_reply": "2025-06-14T10:51:59.885648Z",
     "shell.execute_reply.started": "2025-06-14T10:51:55.029429Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48bc5ddcd3f46dab8b79b316d4d8a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7b86878cda44b7b17df8bee004ce2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "medical_o1_sft.json:   0%|          | 0.00/58.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d4fbccb8ea404a8c2c44542f3ae8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/19704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset,DatasetDict\n",
    "ds = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:51:59.887225Z",
     "iopub.status.busy": "2025-06-14T10:51:59.886965Z",
     "iopub.status.idle": "2025-06-14T10:51:59.917502Z",
     "shell.execute_reply": "2025-06-14T10:51:59.916652Z",
     "shell.execute_reply.started": "2025-06-14T10:51:59.887207Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']['Question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:51:59.918631Z",
     "iopub.status.busy": "2025-06-14T10:51:59.918333Z",
     "iopub.status.idle": "2025-06-14T10:51:59.932311Z",
     "shell.execute_reply": "2025-06-14T10:51:59.931719Z",
     "shell.execute_reply.started": "2025-06-14T10:51:59.918586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_ds(example):\n",
    "    question = example[\"Question\"][0] if isinstance(example[\"Question\"], list) else example[\"Question\"]\n",
    "    long_answer = example[\"Response\"][0] if isinstance(example[\"Response\"], list) else example[\"Response\"]\n",
    "    formatted_text = f\"### Question:\\n{question.strip()}\\n\\n### Answer:\\n{long_answer.strip()}\"\n",
    "    return {\"text\": formatted_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:52:02.171878Z",
     "iopub.status.busy": "2025-06-14T10:52:02.171079Z",
     "iopub.status.idle": "2025-06-14T10:52:03.337585Z",
     "shell.execute_reply": "2025-06-14T10:52:03.336499Z",
     "shell.execute_reply.started": "2025-06-14T10:52:02.171852Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b517eb5950642c39ab908547d2c5b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted_train_dataset = ds[\"train\"].map(format_ds, remove_columns=ds[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:52:03.339059Z",
     "iopub.status.busy": "2025-06-14T10:52:03.338775Z",
     "iopub.status.idle": "2025-06-14T10:52:06.171200Z",
     "shell.execute_reply": "2025-06-14T10:52:06.170348Z",
     "shell.execute_reply.started": "2025-06-14T10:52:03.339040Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\n",
      "\n",
      "### Answer:\n",
      "The specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.\n"
     ]
    }
   ],
   "source": [
    "print(formatted_train_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:52:11.020610Z",
     "iopub.status.busy": "2025-06-14T10:52:11.019962Z",
     "iopub.status.idle": "2025-06-14T10:52:20.890717Z",
     "shell.execute_reply": "2025-06-14T10:52:20.889104Z",
     "shell.execute_reply.started": "2025-06-14T10:52:11.020582Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset with max_seq_length: 512...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52e203c16bd4aa0b3a1a3c43f6db6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_seq_length = 512\n",
    "def tokenize_function(examples):\n",
    "    tokenized= tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "print(f\"Tokenizing dataset with max_seq_length: {max_seq_length}...\")\n",
    "tokenized_train_dataset = formatted_train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,        \n",
    "    remove_columns=[\"text\"],  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:52:36.318677Z",
     "iopub.status.busy": "2025-06-14T10:52:36.317966Z",
     "iopub.status.idle": "2025-06-14T10:52:36.335176Z",
     "shell.execute_reply": "2025-06-14T10:52:36.334546Z",
     "shell.execute_reply.started": "2025-06-14T10:52:36.318643Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 15763\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed=42\n",
    "train_dataset_split = tokenized_train_dataset.train_test_split(test_size=0.2, seed=seed)\n",
    "\n",
    "train_dataset = train_dataset_split[\"train\"]\n",
    "eval_dataset = train_dataset_split[\"test\"] \n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:52:41.889869Z",
     "iopub.status.busy": "2025-06-14T10:52:41.889539Z",
     "iopub.status.idle": "2025-06-14T10:52:41.943566Z",
     "shell.execute_reply": "2025-06-14T10:52:41.942795Z",
     "shell.execute_reply.started": "2025-06-14T10:52:41.889849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=\"./llama-lora-medical\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    run_name=\"llama-1b-lora-medical\",\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    label_names=['labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:52:45.103971Z",
     "iopub.status.busy": "2025-06-14T10:52:45.103378Z",
     "iopub.status.idle": "2025-06-14T10:52:46.267083Z",
     "shell.execute_reply": "2025-06-14T10:52:46.266331Z",
     "shell.execute_reply.started": "2025-06-14T10:52:45.103949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer,default_data_collator\n",
    "trainer=Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=default_data_collator, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:52:46.339938Z",
     "iopub.status.busy": "2025-06-14T10:52:46.339417Z",
     "iopub.status.idle": "2025-06-14T16:40:33.223042Z",
     "shell.execute_reply": "2025-06-14T16:40:33.222189Z",
     "shell.execute_reply.started": "2025-06-14T10:52:46.339915Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3942' max='3942' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3942/3942 5:47:40, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.504100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.299800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.289700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3942, training_loss=0.32137831983561443, metrics={'train_runtime': 20866.4157, 'train_samples_per_second': 1.511, 'train_steps_per_second': 0.189, 'total_flos': 9.441215266789786e+16, 'train_loss': 0.32137831983561443, 'epoch': 2.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:40:39.354838Z",
     "iopub.status.busy": "2025-06-14T16:40:39.354256Z",
     "iopub.status.idle": "2025-06-14T16:40:39.766202Z",
     "shell.execute_reply": "2025-06-14T16:40:39.765400Z",
     "shell.execute_reply.started": "2025-06-14T16:40:39.354815Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final model checkpoint...\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Saving final model checkpoint...\")\n",
    "trainer.save_model() \n",
    "\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T17:43:49.025073Z",
     "iopub.status.busy": "2025-06-14T17:43:49.024463Z",
     "iopub.status.idle": "2025-06-14T17:43:50.125844Z",
     "shell.execute_reply": "2025-06-14T17:43:50.125106Z",
     "shell.execute_reply.started": "2025-06-14T17:43:49.025050Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing raw strings formats for dataset splits\n",
      "Raw inference/evaluation dataset size: 986\n",
      "\n",
      "Tokenizing inference/evaluation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0a766fe4514bfea2328a719a14de26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/986 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized inference/evaluation dataset size: 986\n",
      "Keys in tokenized inference/evaluation dataset: ['input_ids', 'attention_mask', 'reference_answer', 'original_question']\n"
     ]
    }
   ],
   "source": [
    "ds=load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
    "def prepare_raw_string_for_splits(example):\n",
    "    question=example[\"Question\"][0] if isinstance(example[\"Question\"],list) else example[\"Question\"]\n",
    "    long_answer=example[\"Response\"][0] if isinstance(example[\"Response\"],list) else example[\"Response\"]\n",
    "    full_formatted_text_for_training=f\"###Question:\\n{question.strip()}\\n\\n### Answer:\\n{long_answer.strip()}\"\n",
    "    inference_prompt_string=f\"###Question:\\n{question.strip()}\\n\\n### Answer:\"\n",
    "    return {\n",
    "        \"full_formatted_text_for_training\": full_formatted_text_for_training,\n",
    "        \"inference_prompt_string\": inference_prompt_string,\n",
    "        \"reference_answer_string\": long_answer.strip(), \n",
    "        \"original_question_string\": question.strip()\n",
    "    }\n",
    "print(\"preparing raw strings formats for dataset splits\")\n",
    "prepared_raw_dataset=ds[\"train\"].map(prepare_raw_string_for_splits,remove_columns=ds[\"train\"].column_names)\n",
    "seed=42\n",
    "dataset_splits=prepared_raw_dataset.train_test_split(test_size=0.05,seed=seed)\n",
    "train_dataset_raw_strings = dataset_splits[\"train\"]\n",
    "inference_eval_dataset_raw_strings = dataset_splits[\"test\"]\n",
    "print(f\"Raw inference/evaluation dataset size: {len(inference_eval_dataset_raw_strings)}\")\n",
    "max_seq_len=512\n",
    "def tokenize_inference_function(examples):\n",
    "    tokenized_prompt = tokenizer(\n",
    "        examples[\"inference_prompt_string\"], \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors='pt', \n",
    "    )\n",
    "    \n",
    "    tokenized_prompt[\"reference_answer\"] = examples[\"reference_answer_string\"]\n",
    "    tokenized_prompt[\"original_question\"] = examples[\"original_question_string\"]\n",
    "\n",
    "    return tokenized_prompt\n",
    "print(f\"\\nTokenizing inference/evaluation dataset...\")\n",
    "tokenized_inference_eval_dataset = inference_eval_dataset_raw_strings.map(\n",
    "    tokenize_inference_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"full_formatted_text_for_training\", \"inference_prompt_string\",\n",
    "                    \"reference_answer_string\", \"original_question_string\"],\n",
    ")\n",
    "print(f\"Tokenized inference/evaluation dataset size: {len(tokenized_inference_eval_dataset)}\")\n",
    "print(f\"Keys in tokenized inference/evaluation dataset: {tokenized_inference_eval_dataset.column_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T17:26:27.618019Z",
     "iopub.status.busy": "2025-06-14T17:26:27.617473Z",
     "iopub.status.idle": "2025-06-14T17:26:31.612865Z",
     "shell.execute_reply": "2025-06-14T17:26:31.612204Z",
     "shell.execute_reply.started": "2025-06-14T17:26:27.617994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: meta-llama/Llama-3.2-1B...\n",
      "Loading LoRA adapters from: ./llama-lora-medical...\n",
      "Merging LoRA adapters and preparing for inference...\n",
      "Model ready for inference!\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "fine_tuned_model_path = \"./llama-lora-medical\"\n",
    "base_model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "print(f\"Loading base model: {base_model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "tokenizer.padding_side = \"right\" \n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "print(f\"Loading LoRA adapters from: {fine_tuned_model_path}...\")\n",
    "model = PeftModel.from_pretrained(base_model, fine_tuned_model_path)\n",
    "print(\"Merging LoRA adapters and preparing for inference...\")\n",
    "model = model.merge_and_unload() \n",
    "model.eval()\n",
    "print(\"Model ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T17:43:58.436247Z",
     "iopub.status.busy": "2025-06-14T17:43:58.435974Z",
     "iopub.status.idle": "2025-06-14T17:43:58.440562Z",
     "shell.execute_reply": "2025-06-14T17:43:58.439704Z",
     "shell.execute_reply.started": "2025-06-14T17:43:58.436227Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=512,       \n",
    "    do_sample=True,            \n",
    "    temperature=0.7,     \n",
    "    top_p=0.9,                \n",
    "    top_k=40,                \n",
    "    pad_token_id=tokenizer.pad_token_id, \n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T17:44:01.657955Z",
     "iopub.status.busy": "2025-06-14T17:44:01.657154Z",
     "iopub.status.idle": "2025-06-14T18:52:04.898492Z",
     "shell.execute_reply": "2025-06-14T18:52:04.897746Z",
     "shell.execute_reply.started": "2025-06-14T17:44:01.657930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference on evaluation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3fadeb8ee9421299082cb012ec64e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/986 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "predicted_answers = []\n",
    "reference_answers = []\n",
    "original_questions = [] \n",
    "print(\"Starting inference on evaluation dataset...\")\n",
    "for example in tqdm(tokenized_inference_eval_dataset):\n",
    "    inputs={\n",
    "        \"input_ids\":torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(model.device),\n",
    "        \"attention_mask\":torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(model.device)\n",
    "    }\n",
    "    reference_text=example[\"reference_answer\"]\n",
    "    original_q=example[\"original_question\"]\n",
    "    with torch.no_grad():\n",
    "        generated_output=model.generate(\n",
    "            **inputs,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    generated_text=tokenizer.decode(\n",
    "        generated_output[0,inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    if \"### Question:\" in generated_text:\n",
    "        generated_text = generated_text.split(\"### Question:\")[0].strip()\n",
    "    predicted_answers.append(generated_text)\n",
    "    reference_answers.append(reference_text)\n",
    "    original_questions.append(original_q)\n",
    "print(\"Inference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T18:53:52.441384Z",
     "iopub.status.busy": "2025-06-14T18:53:52.440795Z",
     "iopub.status.idle": "2025-06-14T18:53:56.100886Z",
     "shell.execute_reply": "2025-06-14T18:53:56.100012Z",
     "shell.execute_reply.started": "2025-06-14T18:53:52.441364Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T18:54:28.753163Z",
     "iopub.status.busy": "2025-06-14T18:54:28.752885Z",
     "iopub.status.idle": "2025-06-14T18:54:32.351815Z",
     "shell.execute_reply": "2025-06-14T18:54:32.350833Z",
     "shell.execute_reply.started": "2025-06-14T18:54:28.753145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.52.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.31.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.4.26)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert_score) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install  bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T18:54:36.190761Z",
     "iopub.status.busy": "2025-06-14T18:54:36.190427Z",
     "iopub.status.idle": "2025-06-14T18:55:04.722544Z",
     "shell.execute_reply": "2025-06-14T18:55:04.721765Z",
     "shell.execute_reply.started": "2025-06-14T18:54:36.190732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating bert score\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86ea46b5d8d48f7a3c32ed680587efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f846e4cade3e49948a48c3474af3b89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d47c54d2334710b33b69934c82a44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796a02784e82415db4230e5d9cb6078a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808dcadfad7e402fafeefae701a604fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERTScore Results:\n",
      "  Average F1: 0.5487\n",
      "  Average Precision: 0.5823\n",
      "  Average Recall: 0.5281\n",
      "\n",
      "--- Sample Inferences for Manual Inspection ---\n",
      "\n",
      "Question: In the instrument formula for a Gingival Margin Trimmer (GMT) used during cavity preparation, what is the second number representing the angle of the cutting edge when access to the distal gingival margin is achieved?\n",
      "Reference: In the instrument formula for a Gingival Margin Trimmer (GMT) used during cavity preparation, the second number, which represents the angle of the cutting edge, is typically 95 degrees when it is intended for accessing the distal gingival margin. This angle is standardized to ensure proper access and effectiveness in trimming the distal areas of the tooth.\n",
      "Generated: The second number represents the angle of the cutting edge of the Gingival Margin Trimmer.\n",
      "\n",
      "###Explanation: \n",
      "The formula for the Gingival Margin Trimmer is:\n",
      "$$\\frac{0.25}{\\sqrt{3}}\\times\\frac{1}{2}$$\n",
      "\n",
      "In the formula, the second number represents the angle of the cutting edge. It is a fraction of the total angle of the cutting edge, which is 2π. This means that the cutting edge is 0.25 radians (or 12.57 degrees) wide.\n",
      "BERTScore F1 for this example: 0.6029\n",
      "\n",
      "Question: A 67-year-old man comes to the emergency department complaining of severe abdominal pain for the last several hours. The pain is cramp-like in nature, constant, 8/10, and has worsened over time. It is associated with bilious vomiting. He gives a history of episodic right upper abdominal pain for the past few months, mostly after consuming fatty foods, radiating to the tip of the right scapula. He reports no change in bowel habits, bladder habits, or change in weight. His past medical history includes diabetes and hypertension, and he takes hydrochlorothiazide, metformin, ramipril, and atorvastatin. Temperature is 38.2°C (100.8°F), blood pressure is 110/70 mm Hg, pulse is 102/min, respiratory rate is 20/min, and BMI is 23 kg/m2. On physical examination, his abdomen is distended and diffusely tender.\n",
      "Laboratory test\n",
      "Complete blood count\n",
      "Hemoglobin 13 g/dL\n",
      "WBC 16,000/mm3\n",
      "Platelets 150,000/mm3\n",
      "Basic metabolic panel\n",
      "Serum Na+ 148 mEq/L\n",
      "Serum K+ 3.3 mEq/L\n",
      "Serum Cl- 89 mEq/L\n",
      "An abdominal CT scan is shown. What is the most likely underlying cause of this patient’s current presentation?\n",
      "A. Cholelithiasis\n",
      "B. Pancreatitis\n",
      "C. Peptic ulcer disease\n",
      "D. Intestinal adhesion\n",
      "E. Ileal stricture\n",
      "Reference: A. Cholelithiasis\n",
      "\n",
      "The presentation and history are consistent with complications arising from gallstone disease, specifically suggesting gallstone ileus. The history of right upper quadrant pain after fatty meals points to gallstones, and the severe abdominal pain with bilious vomiting and abdominal distension are indicative of a bowel obstruction likely due to a gallstone that has migrated and caused a blockage, i.e., gallstone ileus. The elevated white blood cell count and fever suggest an inflammatory process. On the basis of this information, cholelithiasis is the most likely underlying cause of his current presentation.\n",
      "Generated: Question A\n",
      "\n",
      "This patient's symptoms are most consistent with acute cholecystitis, a disease that typically causes acute, severe pain with a high bilirubin level and elevated ammonia. A cholecystitis is a complication of gallstone disease and is more common in older patients with chronic liver disease. Inflammation of the gallbladder can lead to gallstones, and the presence of gallstones in the setting of cholecystitis would explain the patient's symptoms. Cholecystitis is characterized by the formation of gallstones and inflammation of the gallbladder.\n",
      "\n",
      "The cause of cholecystitis is typically due to inflammation and infection of the gallbladder, which is characterized by elevated bilirubin and ammonia levels. Cholecystitis is also associated with cholangitis, which is inflammation of the biliary tree, and may be associated with elevated liver enzymes.\n",
      "\n",
      "Gallstone disease is more common in older patients with chronic liver disease. The most common cause of cholangitis is gallstone disease, which is more common in older patients. Gallstones are small, round, and calcified, and are more common in patients with gallstone disease. They can be impacted in the gallbladder, which is more common in older patients with chronic liver disease. Patients with gallstone disease are at an increased risk of cholecystitis, which is characterized by inflammation and infection of the gallbladder, as well as gallstones. \n",
      "\n",
      "Cholangitis is inflammation of the biliary tree, which is more common in patients with gallstone disease. Patients with gallstone disease are at an increased risk of cholangitis, which is characterized by inflammation and infection of the biliary tree. Inflammation and infection of the gallbladder are both common in patients with gallstone disease, and these features are typically associated with gallstones. \n",
      "\n",
      "The patient's symptoms of abdominal pain, nausea, and vomiting are consistent with acute cholecystitis. The presence of a gallstone is also consistent with acute cholecystitis. The patient's history of gallstone disease is also consistent with acute cholecystitis, as it is more common in patients with chronic liver disease. \n",
      "\n",
      "The CT scan is consistent with acute cholecystitis. Gallstones are more common in patients with gallstone disease. Gallstones are calcified, and patients with gallstone disease are at an increased risk of cholangitis, which is characterized by inflammation and infection of the biliary tree. Patients with gallstone disease are also at an\n",
      "BERTScore F1 for this example: 0.5751\n",
      "\n",
      "Question: In a diving accident that results in the severing of the spinal cord below the sixth cervical vertebra, which muscle would be affected due to its innervation by spinal levels below this point?\n",
      "Reference: In a diving accident where the spinal cord is severed below the sixth cervical vertebra (C6), muscles that depend on innervation from spinal levels below this point would be affected. The intrinsic muscles of the hand, such as the interossei, are among those significantly impacted. These muscles are primarily innervated by the C8 and T1 nerves, which are below the injury site at C6. As a result, fine motor functions and intricate movements of the hand and fingers would be compromised.\n",
      "Generated: 6th cervical diaphragm\n",
      "The diaphragm is a flat sheet of muscle that forms the diaphragm and separates the thoracic and abdominal cavities. The diaphragm is innervated by the thoracic spinal cord. In a spinal cord injury, it is severed in the cervical spinal cord, resulting in damage to the diaphragm.\n",
      "BERTScore F1 for this example: 0.5647\n",
      "\n",
      "Question: In a study conducted on patients with confirmed HIV infections in Mumbai, where 100 individuals out of 200 were randomly selected to receive a new drug mixed with orange juice, and neither the medical staff nor the patients knew who received the drug, what is the research method used to ensure unbiased results?\n",
      "Reference: The research method used in this study to ensure unbiased results is called a double-blind, randomized controlled trial. In this type of study, participants are randomly assigned to either the group receiving the treatment or the control group. Additionally, neither the participants nor the researchers know who is receiving the treatment, which helps prevent bias in administering the treatment or reporting results. This approach is considered the gold standard for clinical trials because it minimizes biases and allows for a more accurate evaluation of the drug's effectiveness.\n",
      "Generated: Home\n",
      "A. Randomized control trial\n",
      "B. Case control study\n",
      "C. Case study\n",
      "D. Cross-sectional study\n",
      "E. Cross over study\n",
      "Answer: A\n",
      "BERTScore F1 for this example: 0.4381\n",
      "\n",
      "Question: What is the diagnosis for a homosexual person who feels imposed by a female body and experiences persistent discomfort with their sex?\n",
      "Reference: The situation you're describing is indicative of gender dysphoria. Gender dysphoria is the distress or discomfort that may occur when a person's gender identity does not align with their sex assigned at birth. In this case, the person is expressing significant discomfort with their female body, which can be a key sign of gender dysphoria. Their sexual orientation, being homosexual, relates to who they are attracted to and doesn't directly impact their feelings about their gender identity. Addressing gender dysphoria often involves exploring one's gender identity with professionals who can provide support and guidance tailored to the individual's experiences and needs.\n",
      "Generated: 1. Female sexual dysfunction\n",
      "1. Homosexuality\n",
      "2. Sexual confusion\n",
      "3. Sexual dysfunction\n",
      "4. Sexual attraction to males\n",
      "5. Sexual dysfunction\n",
      "6. Sexual dysfunction\n",
      "7. Sexual dysfunction\n",
      "8. Sexual dysfunction\n",
      "9. Sexual dysfunction\n",
      "10. Sexual dysfunction\n",
      "11. Sexual dysfunction\n",
      "12. Sexual dysfunction\n",
      "13. Sexual dysfunction\n",
      "14. Sexual dysfunction\n",
      "15. Sexual dysfunction\n",
      "16. Sexual dysfunction\n",
      "17. Sexual dysfunction\n",
      "18. Sexual dysfunction\n",
      "19. Sexual dysfunction\n",
      "20. Sexual dysfunction\n",
      "21. Sexual dysfunction\n",
      "22. Sexual dysfunction\n",
      "23. Sexual dysfunction\n",
      "24. Sexual dysfunction\n",
      "25. Sexual dysfunction\n",
      "26. Sexual dysfunction\n",
      "27. Sexual dysfunction\n",
      "28. Sexual dysfunction\n",
      "29. Sexual dysfunction\n",
      "30. Sexual dysfunction\n",
      "31. Sexual dysfunction\n",
      "32. Sexual dysfunction\n",
      "33. Sexual dysfunction\n",
      "34. Sexual dysfunction\n",
      "35. Sexual dysfunction\n",
      "36. Sexual dysfunction\n",
      "37. Sexual dysfunction\n",
      "38. Sexual dysfunction\n",
      "39. Sexual dysfunction\n",
      "40. Sexual dysfunction\n",
      "41. Sexual dysfunction\n",
      "42. Sexual dysfunction\n",
      "43. Sexual dysfunction\n",
      "44. Sexual dysfunction\n",
      "45. Sexual dysfunction\n",
      "46. Sexual dysfunction\n",
      "47. Sexual dysfunction\n",
      "48. Sexual dysfunction\n",
      "49. Sexual dysfunction\n",
      "50. Sexual dysfunction\n",
      "51. Sexual dysfunction\n",
      "52. Sexual dysfunction\n",
      "53. Sexual dysfunction\n",
      "54. Sexual dysfunction\n",
      "55. Sexual dysfunction\n",
      "56. Sexual dysfunction\n",
      "57. Sexual dysfunction\n",
      "58. Sexual dysfunction\n",
      "59. Sexual dysfunction\n",
      "60. Sexual dysfunction\n",
      "61. Sexual dysfunction\n",
      "62. Sexual dysfunction\n",
      "63. Sexual dysfunction\n",
      "64. Sexual dysfunction\n",
      "65. Sexual dysfunction\n",
      "66. Sexual dysfunction\n",
      "67. Sexual dysfunction\n",
      "68. Sexual dysfunction\n",
      "69. Sexual dysfunction\n",
      "70. Sexual dysfunction\n",
      "71. Sexual dysfunction\n",
      "72. Sexual dysfunction\n",
      "73. Sexual dysfunction\n",
      "74. Sexual dysfunction\n",
      "75. Sexual dysfunction\n",
      "76. Sexual dysfunction\n",
      "77. Sexual dysfunction\n",
      "78. Sexual dysfunction\n",
      "79. Sexual dysfunction\n",
      "80. Sexual dysfunction\n",
      "81. Sexual dysfunction\n",
      "82. Sexual dysfunction\n",
      "83. Sexual dysfunction\n",
      "84. Sexual dysfunction\n",
      "85. Sexual dysfunction\n",
      "86. Sexual dysfunction\n",
      "87. Sexual dysfunction\n",
      "88. Sexual dysfunction\n",
      "89. Sexual dysfunction\n",
      "90. Sexual dysfunction\n",
      "91. Sexual dysfunction\n",
      "92. Sexual dysfunction\n",
      "93. Sexual dysfunction\n",
      "94. Sexual dysfunction\n",
      "95. Sexual dysfunction\n",
      "96. Sexual dysfunction\n",
      "97. Sexual dysfunction\n",
      "98. Sexual dysfunction\n",
      "99. Sexual dysfunction\n",
      "100. Sexual dysfunction\n",
      "BERTScore F1 for this example: 0.3754\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np \n",
    "print(\"calculating bert score\")\n",
    "bertscore=evaluate.load(\"bertscore\")\n",
    "results = bertscore.compute(\n",
    "    predictions=predicted_answers,\n",
    "    references=reference_answers,\n",
    "    model_type=\"bert-base-uncased\", \n",
    "    lang=\"en\", \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    ")\n",
    "average_f1 = np.mean(results[\"f1\"])\n",
    "print(f\"\\nBERTScore Results:\")\n",
    "print(f\"  Average F1: {average_f1:.4f}\")\n",
    "print(f\"  Average Precision: {np.mean(results['precision']):.4f}\")\n",
    "print(f\"  Average Recall: {np.mean(results['recall']):.4f}\")\n",
    "print(\"\\n--- Sample Inferences for Manual Inspection ---\")\n",
    "for i in range(min(5, len(original_questions))): \n",
    "    print(f\"\\nQuestion: {original_questions[i]}\")\n",
    "    print(f\"Reference: {reference_answers[i]}\")\n",
    "    print(f\"Generated: {predicted_answers[i]}\")\n",
    "    print(f\"BERTScore F1 for this example: {results['f1'][i]:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUSHING TO THE HUGGINGFACE HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T17:39:56.524732Z",
     "iopub.status.busy": "2025-06-14T17:39:56.524425Z",
     "iopub.status.idle": "2025-06-14T17:40:32.737224Z",
     "shell.execute_reply": "2025-06-14T17:40:32.736653Z",
     "shell.execute_reply.started": "2025-06-14T17:39:56.524710Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model and tokenizer to Hugging Face Hub: mokshaik/llama-3-2-1b-medical-qa-lora-finetuned...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de671e40754845ceb28a3a0bcefa43a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896e58b787f54b78aa6d99999afeb47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93969095a4a415590ae391f2125aa5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mokshaik/llama-3-2-1b-medical-qa-lora-finetuned/commit/c9dfe6143cd8a0f57602f5cce41fa83944d34ce1', commit_message='Add tokenizer for Llama 3.2 1B medical QA fine-tune', commit_description='', oid='c9dfe6143cd8a0f57602f5cce41fa83944d34ce1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mokshaik/llama-3-2-1b-medical-qa-lora-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='mokshaik/llama-3-2-1b-medical-qa-lora-finetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_repo_id=\"mokshaik/llama-3-2-1b-medical-qa-lora-finetuned\"\n",
    "print(f\"Uploading model and tokenizer to Hugging Face Hub: {hf_repo_id}...\")\n",
    "model.push_to_hub(hf_repo_id,commit_message=\"Initial release of Llama 3.2 1B mdeical QA Lora Finetune\")\n",
    "tokenizer.push_to_hub(hf_repo_id, commit_message=\"Add tokenizer for Llama 3.2 1B medical QA fine-tune\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING FROM HUGGING FACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T19:00:27.031319Z",
     "iopub.status.busy": "2025-06-14T19:00:27.030470Z",
     "iopub.status.idle": "2025-06-14T19:00:27.180837Z",
     "shell.execute_reply": "2025-06-14T19:00:27.180051Z",
     "shell.execute_reply.started": "2025-06-14T19:00:27.031292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T19:11:02.666264Z",
     "iopub.status.busy": "2025-06-14T19:11:02.665330Z",
     "iopub.status.idle": "2025-06-14T19:11:26.185023Z",
     "shell.execute_reply": "2025-06-14T19:11:26.184308Z",
     "shell.execute_reply.started": "2025-06-14T19:11:02.666230Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer from Hugging Face Hub: mokshaik/llama-3-2-1b-medical-qa-lora-finetuned...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b2cc72e4f94a73aa9c028d776c8e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d944a82f9a4e26a06cc11bc12083e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bace284433c4bccab3e456f9fbe391e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f27e205b5b429baf2916be2a58b8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c7502dd9844acb8ee96eb42c83a202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf258ce58234766bf9ae88e85465a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'bos_token_id': 128000}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully for local inference!\n",
      "Performing local inference...\n",
      "\n",
      "MODEL RESPONSE IS:\n",
      "1\n",
      "The symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg suggest that the patient is experiencing a stroke. A stroke can cause damage to the arteries of the brain, leading to a decrease in blood flow to the brain and potentially resulting in weakness and paralysis. This condition can occur due to a variety of underlying causes, such as atherosclerosis, heart disease, or blood clots. To rule out other potential causes, it is crucial to conduct a thorough medical history and perform a detailed physical examination, including a neurological evaluation. In the context of a recent long-distance trip, the potential underlying cause of the stroke could be due to blood clots formed during the trip. This is why it is important to conduct a thorough medical history and perform a detailed physical examination, including a neurological evaluation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"mokshaik/llama-3-2-1b-medical-qa-lora-finetuned\"\n",
    "print(f\"Loading model and tokenizer from Hugging Face Hub: {model_id}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    load_in_4bit=True,         \n",
    "    device_map=\"auto\"          \n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"Model loaded successfully for local inference!\")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=512,       \n",
    "    do_sample=True,\n",
    "    temperature=0.7,          \n",
    "    top_p=0.9,                 \n",
    "    top_k=50,                 \n",
    "    pad_token_id=tokenizer.pad_token_id, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    ")\n",
    "\n",
    "prompt = \"\"\"### Question:\n",
    "Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\n",
    "\n",
    "### Answer: \"\"\"\n",
    "\n",
    "\n",
    "print(\"Performing local inference...\")\n",
    "try:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        generated_output = model.generate(\n",
    "            **inputs,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "\n",
    "    response_text = tokenizer.decode(\n",
    "        generated_output[0, inputs[\"input_ids\"].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "    if \"### Question:\" in response_text:\n",
    "        response_text = response_text.split(\"### Question:\")[0].strip()\n",
    "\n",
    "    print(\"\\nMODEL RESPONSE IS:\")\n",
    "    print(response_text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during inference: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
