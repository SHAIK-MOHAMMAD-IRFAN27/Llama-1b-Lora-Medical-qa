{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y peft transformers accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:47:18.530035Z","iopub.execute_input":"2025-06-14T10:47:18.530224Z","iopub.status.idle":"2025-06-14T10:47:24.236881Z","shell.execute_reply.started":"2025-06-14T10:47:18.530207Z","shell.execute_reply":"2025-06-14T10:47:24.235929Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: peft 0.14.0\nUninstalling peft-0.14.0:\n  Successfully uninstalled peft-0.14.0\nFound existing installation: transformers 4.51.3\nUninstalling transformers-4.51.3:\n  Successfully uninstalled transformers-4.51.3\nFound existing installation: accelerate 1.5.2\nUninstalling accelerate-1.5.2:\n  Successfully uninstalled accelerate-1.5.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q transformers peft bitsandbytes accelerate trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:47:24.238444Z","iopub.execute_input":"2025-06-14T10:47:24.238777Z","iopub.status.idle":"2025-06-14T10:48:53.032527Z","shell.execute_reply.started":"2025-06-14T10:47:24.238750Z","shell.execute_reply":"2025-06-14T10:48:53.031825Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## LOAD DEPENDENCIES","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:48:53.033593Z","iopub.execute_input":"2025-06-14T10:48:53.033909Z","iopub.status.idle":"2025-06-14T10:49:18.328115Z","shell.execute_reply.started":"2025-06-14T10:48:53.033877Z","shell.execute_reply":"2025-06-14T10:49:18.327295Z"}},"outputs":[{"name":"stderr","text":"2025-06-14 10:49:05.061780: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749898145.266547      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749898145.326780      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### 4 BIT QUANTIZATION","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=False,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:51:26.430070Z","iopub.execute_input":"2025-06-14T10:51:26.430422Z","iopub.status.idle":"2025-06-14T10:51:26.436372Z","shell.execute_reply.started":"2025-06-14T10:51:26.430399Z","shell.execute_reply":"2025-06-14T10:51:26.435650Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"model_id = \"meta-llama/Llama-3.2-1B\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:51:26.824536Z","iopub.execute_input":"2025-06-14T10:51:26.824858Z","iopub.status.idle":"2025-06-14T10:51:26.828759Z","shell.execute_reply.started":"2025-06-14T10:51:26.824836Z","shell.execute_reply":"2025-06-14T10:51:26.827942Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### HUGGINGFACE SECRET TOKEN","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T19:08:27.492652Z","iopub.execute_input":"2025-06-14T19:08:27.493233Z","iopub.status.idle":"2025-06-14T19:08:27.644571Z","shell.execute_reply.started":"2025-06-14T19:08:27.493210Z","shell.execute_reply":"2025-06-14T19:08:27.643826Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# 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\nfrom huggingface_hub import login\nlogin(secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:51:27.559323Z","iopub.execute_input":"2025-06-14T10:51:27.559716Z","iopub.status.idle":"2025-06-14T10:51:27.669447Z","shell.execute_reply.started":"2025-06-14T10:51:27.559681Z","shell.execute_reply":"2025-06-14T10:51:27.668892Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(f\"Loading tokenizer for {model_id}...\")\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token # Essential for consistent padding during training\ntokenizer.padding_side = \"right\" # Helps with causal language modeling\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:51:29.560375Z","iopub.execute_input":"2025-06-14T10:51:29.560720Z","iopub.status.idle":"2025-06-14T10:51:31.452933Z","shell.execute_reply.started":"2025-06-14T10:51:29.560696Z","shell.execute_reply":"2025-06-14T10:51:31.452255Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer for meta-llama/Llama-3.2-1B...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dee2f6dc241f4601b54e7a802638b384"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68bb55a4056240f7aeaa7105fc3bc5a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9e3cca5a22843bb85db08ef9df434dd"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"print(f\"Loading model {model_id} in 4-bit precision...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map={\"\": 0}, \n    torch_dtype=torch.bfloat16, # This sets the default compute dtype\n    trust_remote_code=True, # May be needed for some newer models\n)\n\n# Disable cache to save memory during training and ensure reproducibility\nmodel.config.use_cache = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:51:31.454135Z","iopub.execute_input":"2025-06-14T10:51:31.454420Z","iopub.status.idle":"2025-06-14T10:51:46.468121Z","shell.execute_reply.started":"2025-06-14T10:51:31.454396Z","shell.execute_reply":"2025-06-14T10:51:46.467255Z"}},"outputs":[{"name":"stdout","text":"Loading model meta-llama/Llama-3.2-1B in 4-bit precision...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4822a5e99cc445689a261e3a48316dd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9888469dc18e4129b4ab9207c6ce4e10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7f974903ba54737a6ef502eeb9e7a00"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"print(\"Preparing model for k-bit training (QLoRA)...\")\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:51:46.469526Z","iopub.execute_input":"2025-06-14T10:51:46.470000Z","iopub.status.idle":"2025-06-14T10:51:46.503462Z","shell.execute_reply.started":"2025-06-14T10:51:46.469970Z","shell.execute_reply":"2025-06-14T10:51:46.502568Z"}},"outputs":[{"name":"stdout","text":"Preparing model for k-bit training (QLoRA)...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16, # LoRA attention dimension (try 8, 16, 32 depending on needs)\n    lora_alpha=16, # Alpha parameter for LoRA scaling (often same as r)\n    lora_dropout=0.05, # Dropout for LoRA layers\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\n        \"q_proj\", \"v_proj\", # Feed-forward network projections\n    ]\n)\nprint(\"Attaching LoRA adapters to the model...\")\nmodel = get_peft_model(model, lora_config)\n# This will show you the exact number of trainable parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:51:46.504481Z","iopub.execute_input":"2025-06-14T10:51:46.504927Z","iopub.status.idle":"2025-06-14T10:51:55.027653Z","shell.execute_reply.started":"2025-06-14T10:51:46.504901Z","shell.execute_reply":"2025-06-14T10:51:55.026837Z"}},"outputs":[{"name":"stdout","text":"Attaching LoRA adapters to the model...\ntrainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from datasets import load_dataset,DatasetDict\n\n# Login using e.g. `huggingface-cli login` to access this dataset\nds = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:51:55.029152Z","iopub.execute_input":"2025-06-14T10:51:55.029449Z","iopub.status.idle":"2025-06-14T10:51:59.886251Z","shell.execute_reply.started":"2025-06-14T10:51:55.029429Z","shell.execute_reply":"2025-06-14T10:51:59.885648Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e48bc5ddcd3f46dab8b79b316d4d8a1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"medical_o1_sft.json:   0%|          | 0.00/58.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d7b86878cda44b7b17df8bee004ce2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/19704 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0d4fbccb8ea404a8c2c44542f3ae8e1"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"ds['train']['Question'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:51:59.886965Z","iopub.execute_input":"2025-06-14T10:51:59.887225Z","iopub.status.idle":"2025-06-14T10:51:59.917502Z","shell.execute_reply.started":"2025-06-14T10:51:59.887207Z","shell.execute_reply":"2025-06-14T10:51:59.916652Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?'"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"def format_ds(example):\n    question = example[\"Question\"][0] if isinstance(example[\"Question\"], list) else example[\"Question\"]\n    long_answer = example[\"Response\"][0] if isinstance(example[\"Response\"], list) else example[\"Response\"]\n    formatted_text = f\"### Question:\\n{question.strip()}\\n\\n### Answer:\\n{long_answer.strip()}\"\n    return {\"text\": formatted_text}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:51:59.918333Z","iopub.execute_input":"2025-06-14T10:51:59.918631Z","iopub.status.idle":"2025-06-14T10:51:59.932311Z","shell.execute_reply.started":"2025-06-14T10:51:59.918586Z","shell.execute_reply":"2025-06-14T10:51:59.931719Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"formatted_train_dataset = ds[\"train\"].map(format_ds, remove_columns=ds[\"train\"].column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:52:02.171079Z","iopub.execute_input":"2025-06-14T10:52:02.171878Z","iopub.status.idle":"2025-06-14T10:52:03.337585Z","shell.execute_reply.started":"2025-06-14T10:52:02.171852Z","shell.execute_reply":"2025-06-14T10:52:03.336499Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19704 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b517eb5950642c39ab908547d2c5b0d"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"print(formatted_train_dataset[0][\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:52:03.338775Z","iopub.execute_input":"2025-06-14T10:52:03.339059Z","iopub.status.idle":"2025-06-14T10:52:06.171200Z","shell.execute_reply.started":"2025-06-14T10:52:03.339040Z","shell.execute_reply":"2025-06-14T10:52:06.170348Z"}},"outputs":[{"name":"stdout","text":"### Question:\nGiven the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\n\n### Answer:\nThe specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"max_seq_length = 512 # Adjust based on your VRAM and average example length\n\n# 2. Create the tokenization function\ndef tokenize_function(examples):\n    # This function will be applied to each example (or batch of examples) in your dataset.\n    # We pass the 'text' column from our formatted dataset.\n    # 'truncation=True' will cut off any examples longer than max_seq_length.\n    # We do NOT set padding here, as SFTTrainer will handle dynamic padding later.\n    tokenized= tokenizer(\n        examples[\"text\"],\n        padding='max_length',\n        truncation=True,\n        max_length=max_seq_length,\n        return_tensors='pt',\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n    return tokenized\n# 3. Apply the tokenization to your dataset\nprint(f\"Tokenizing dataset with max_seq_length: {max_seq_length}...\")\ntokenized_train_dataset = formatted_train_dataset.map(\n    tokenize_function,\n    batched=True,        # Process examples in batches, which is faster for tokenization\n    remove_columns=[\"text\"], # Remove the original 'text' column to save memory         # Use multiple processes for faster tokenization (adjust based on CPU cores)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:52:11.019962Z","iopub.execute_input":"2025-06-14T10:52:11.020610Z","iopub.status.idle":"2025-06-14T10:52:20.890717Z","shell.execute_reply.started":"2025-06-14T10:52:11.020582Z","shell.execute_reply":"2025-06-14T10:52:20.889104Z"}},"outputs":[{"name":"stdout","text":"Tokenizing dataset with max_seq_length: 512...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19704 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d52e203c16bd4aa0b3a1a3c43f6db6ff"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"seed=42\ntrain_dataset_split = tokenized_train_dataset.train_test_split(test_size=0.2, seed=seed)\n\n# Access your new training and evaluation splits\ntrain_dataset = train_dataset_split[\"train\"]\neval_dataset = train_dataset_split[\"test\"] # 'test' is the default key for the test split\ntrain_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:52:36.317966Z","iopub.execute_input":"2025-06-14T10:52:36.318677Z","iopub.status.idle":"2025-06-14T10:52:36.335176Z","shell.execute_reply.started":"2025-06-14T10:52:36.318643Z","shell.execute_reply":"2025-06-14T10:52:36.334546Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 15763\n})"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"# TRAINING LOOP","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\ntraining_args=TrainingArguments(\n    output_dir=\"./llama-lora-medical\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=2,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=2,\n    report_to=\"none\",\n    remove_unused_columns=False,\n    run_name=\"llama-1b-lora-medical\",\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"cosine\",\n    label_names=['labels']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:52:41.889539Z","iopub.execute_input":"2025-06-14T10:52:41.889869Z","iopub.status.idle":"2025-06-14T10:52:41.943566Z","shell.execute_reply.started":"2025-06-14T10:52:41.889849Z","shell.execute_reply":"2025-06-14T10:52:41.942795Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from transformers import Trainer,default_data_collator\ntrainer=Trainer(\n    model=model,\n    train_dataset=train_dataset,\n    args=training_args,\n    data_collator=default_data_collator, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:52:45.103378Z","iopub.execute_input":"2025-06-14T10:52:45.103971Z","iopub.status.idle":"2025-06-14T10:52:46.267083Z","shell.execute_reply.started":"2025-06-14T10:52:45.103949Z","shell.execute_reply":"2025-06-14T10:52:46.266331Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:52:46.339417Z","iopub.execute_input":"2025-06-14T10:52:46.339938Z","iopub.status.idle":"2025-06-14T16:40:33.223042Z","shell.execute_reply.started":"2025-06-14T10:52:46.339915Z","shell.execute_reply":"2025-06-14T16:40:33.222189Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3942' max='3942' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3942/3942 5:47:40, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.504100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.298200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.299800</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.296000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.296700</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.288700</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.289700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3942, training_loss=0.32137831983561443, metrics={'train_runtime': 20866.4157, 'train_samples_per_second': 1.511, 'train_steps_per_second': 0.189, 'total_flos': 9.441215266789786e+16, 'train_loss': 0.32137831983561443, 'epoch': 2.0})"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# Assuming your trainer object is named 'trainer'\nprint(\"Saving final model checkpoint...\")\ntrainer.save_model() # This saves the LoRA adapters\n\n# Optionally, if you want to save the final training state (optimizer, scheduler, etc.)\n# trainer.save_state()\nprint(\"Model saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T16:40:39.354256Z","iopub.execute_input":"2025-06-14T16:40:39.354838Z","iopub.status.idle":"2025-06-14T16:40:39.766202Z","shell.execute_reply.started":"2025-06-14T16:40:39.354815Z","shell.execute_reply":"2025-06-14T16:40:39.765400Z"}},"outputs":[{"name":"stdout","text":"Saving final model checkpoint...\nModel saved!\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# INFERENCE LOOP","metadata":{}},{"cell_type":"code","source":"ds=load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\ndef prepare_raw_string_for_splits(example):\n    question=example[\"Question\"][0] if isinstance(example[\"Question\"],list) else example[\"Question\"]\n    long_answer=example[\"Response\"][0] if isinstance(example[\"Response\"],list) else example[\"Response\"]\n    full_formatted_text_for_training=f\"###Question:\\n{question.strip()}\\n\\n### Answer:\\n{long_answer.strip()}\"\n    inference_prompt_string=f\"###Question:\\n{question.strip()}\\n\\n### Answer:\"\n    return {\n        \"full_formatted_text_for_training\": full_formatted_text_for_training,\n        \"inference_prompt_string\": inference_prompt_string,\n        \"reference_answer_string\": long_answer.strip(), # Keep this as a string for BERTScore\n        \"original_question_string\": question.strip()\n    }\nprint(\"preparing raw strings formats for dataset splits\")\nprepared_raw_dataset=ds[\"train\"].map(prepare_raw_string_for_splits,remove_columns=ds[\"train\"].column_names)\nseed=42\ndataset_splits=prepared_raw_dataset.train_test_split(test_size=0.05,seed=seed)\ntrain_dataset_raw_strings = dataset_splits[\"train\"]\ninference_eval_dataset_raw_strings = dataset_splits[\"test\"]\nprint(f\"Raw inference/evaluation dataset size: {len(inference_eval_dataset_raw_strings)}\")\nmax_seq_len=512\ndef tokenize_inference_function(examples):\n    tokenized_prompt = tokenizer(\n        examples[\"inference_prompt_string\"], # Only tokenize the prompt part\n        padding='max_length',\n        truncation=True,\n        max_length=max_seq_length,\n        return_tensors='pt', # Return PyTorch tensors\n    )\n    \n    # Crucially, keep the string references for BERTScore later\n    tokenized_prompt[\"reference_answer\"] = examples[\"reference_answer_string\"]\n    tokenized_prompt[\"original_question\"] = examples[\"original_question_string\"]\n\n    return tokenized_prompt\nprint(f\"\\nTokenizing inference/evaluation dataset...\")\n# Map the inference split. Keep the original string answers.\ntokenized_inference_eval_dataset = inference_eval_dataset_raw_strings.map(\n    tokenize_inference_function,\n    batched=True,\n    # Remove the raw string columns after tokenization, except for what we specifically kept\n    remove_columns=[\"full_formatted_text_for_training\", \"inference_prompt_string\",\n                    \"reference_answer_string\", \"original_question_string\"],\n)\nprint(f\"Tokenized inference/evaluation dataset size: {len(tokenized_inference_eval_dataset)}\")\nprint(f\"Keys in tokenized inference/evaluation dataset: {tokenized_inference_eval_dataset.column_names}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T17:43:49.024463Z","iopub.execute_input":"2025-06-14T17:43:49.025073Z","iopub.status.idle":"2025-06-14T17:43:50.125844Z","shell.execute_reply.started":"2025-06-14T17:43:49.025050Z","shell.execute_reply":"2025-06-14T17:43:50.125106Z"}},"outputs":[{"name":"stdout","text":"preparing raw strings formats for dataset splits\nRaw inference/evaluation dataset size: 986\n\nTokenizing inference/evaluation dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/986 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f0a766fe4514bfea2328a719a14de26"}},"metadata":{}},{"name":"stdout","text":"Tokenized inference/evaluation dataset size: 986\nKeys in tokenized inference/evaluation dataset: ['input_ids', 'attention_mask', 'reference_answer', 'original_question']\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\nimport torch\nfine_tuned_model_path = \"./llama-lora-medical\"\nbase_model_id = \"meta-llama/Llama-3.2-1B\"\nprint(f\"Loading base model: {base_model_id}...\")\n# Load the tokenizer again, ensuring it's the one used during training\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\ntokenizer.pad_token = tokenizer.eos_token # Ensure this is consistent for generation\ntokenizer.padding_side = \"right\" # Ensure this is consistent\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    quantization_config=bnb_config,\n    device_map={\"\": 0}, \n    torch_dtype=torch.bfloat16, # This sets the default compute dtype\n    trust_remote_code=True, # May be needed for some newer models\n)\nprint(f\"Loading LoRA adapters from: {fine_tuned_model_path}...\")\n# Load the LoRA adapters onto the base model\nmodel = PeftModel.from_pretrained(base_model, fine_tuned_model_path)\nprint(\"Merging LoRA adapters and preparing for inference...\")\nmodel = model.merge_and_unload() # This returns a new, merged model\nmodel.eval()# Set the model to evaluation mode\nprint(\"Model ready for inference!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T17:26:27.617473Z","iopub.execute_input":"2025-06-14T17:26:27.618019Z","iopub.status.idle":"2025-06-14T17:26:31.612865Z","shell.execute_reply.started":"2025-06-14T17:26:27.617994Z","shell.execute_reply":"2025-06-14T17:26:31.612204Z"}},"outputs":[{"name":"stdout","text":"Loading base model: meta-llama/Llama-3.2-1B...\nLoading LoRA adapters from: ./llama-lora-medical...\nMerging LoRA adapters and preparing for inference...\nModel ready for inference!\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"generation_config = GenerationConfig(\n    max_new_tokens=512,        # Max length of the generated answer (adjust based on expected answer length)\n    do_sample=True,            # Whether to use sampling (True) or greedy decoding (False)\n    temperature=0.7,           # Controls randomness (lower means less random)\n    top_p=0.9,                 # Nucleus sampling: sample from top_p probability mass\n    top_k=40,                  # Top-k sampling: sample from top_k tokens\n    pad_token_id=tokenizer.pad_token_id, # Essential for batch inference\n    eos_token_id=tokenizer.eos_token_id, # Stop generation when EOS token is produced\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T17:43:58.435974Z","iopub.execute_input":"2025-06-14T17:43:58.436247Z","iopub.status.idle":"2025-06-14T17:43:58.440562Z","shell.execute_reply.started":"2025-06-14T17:43:58.436227Z","shell.execute_reply":"2025-06-14T17:43:58.439704Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"from tqdm.auto import tqdm \npredicted_answers = []\nreference_answers = []\noriginal_questions = [] \nprint(\"Starting inference on evaluation dataset...\")\nfor example in tqdm(tokenized_inference_eval_dataset):\n    inputs={\n        \"input_ids\":torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(model.device),\n        \"attention_mask\":torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(model.device)\n    }\n    reference_text=example[\"reference_answer\"]\n    original_q=example[\"original_question\"]\n    with torch.no_grad():\n        generated_output=model.generate(\n            **inputs,\n            generation_config=generation_config,\n        )\n    generated_text=tokenizer.decode(\n        generated_output[0,inputs[\"input_ids\"].shape[1]:],\n        skip_special_tokens=True\n    ).strip()\n    if \"### Question:\" in generated_text:\n        generated_text = generated_text.split(\"### Question:\")[0].strip()\n    predicted_answers.append(generated_text)\n    reference_answers.append(reference_text)\n    original_questions.append(original_q)\nprint(\"Inference complete!\")","metadata":{"execution":{"iopub.status.busy":"2025-06-14T17:44:01.657154Z","iopub.execute_input":"2025-06-14T17:44:01.657955Z","iopub.status.idle":"2025-06-14T18:52:04.898492Z","shell.execute_reply.started":"2025-06-14T17:44:01.657930Z","shell.execute_reply":"2025-06-14T18:52:04.897746Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Starting inference on evaluation dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/986 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da3fadeb8ee9421299082cb012ec64e1"}},"metadata":{}},{"name":"stdout","text":"Inference complete!\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:53:52.440795Z","iopub.execute_input":"2025-06-14T18:53:52.441384Z","iopub.status.idle":"2025-06-14T18:53:56.100886Z","shell.execute_reply.started":"2025-06-14T18:53:52.441364Z","shell.execute_reply":"2025-06-14T18:53:56.100012Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"!pip install  bert_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:54:28.752885Z","iopub.execute_input":"2025-06-14T18:54:28.753163Z","iopub.status.idle":"2025-06-14T18:54:32.351815Z","shell.execute_reply.started":"2025-06-14T18:54:28.753145Z","shell.execute_reply":"2025-06-14T18:54:32.350833Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.52.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.31.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.4.26)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert_score) (1.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"import evaluate\nimport numpy as np \nprint(\"calculating bert score\")\nbertscore=evaluate.load(\"bertscore\")\n# Compute BERTScore\n# BERTScore needs to download a pre-trained BERT model for computing embeddings.\n# This happens once. 'bert-base-uncased' is a common choice.\nresults = bertscore.compute(\n    predictions=predicted_answers,\n    references=reference_answers,\n    model_type=\"bert-base-uncased\", # Use a specific BERT model for contextual embeddings\n    lang=\"en\", # Specify the language of your text\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\" # Use GPU for faster BERTScore computation if available\n)\naverage_f1 = np.mean(results[\"f1\"])\nprint(f\"\\nBERTScore Results:\")\nprint(f\"  Average F1: {average_f1:.4f}\")\nprint(f\"  Average Precision: {np.mean(results['precision']):.4f}\")\nprint(f\"  Average Recall: {np.mean(results['recall']):.4f}\")\nprint(\"\\n--- Sample Inferences for Manual Inspection ---\")\nfor i in range(min(5, len(original_questions))): # Print details for the first 5 examples\n    print(f\"\\nQuestion: {original_questions[i]}\")\n    print(f\"Reference: {reference_answers[i]}\")\n    print(f\"Generated: {predicted_answers[i]}\")\n    print(f\"BERTScore F1 for this example: {results['f1'][i]:.4f}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:54:36.190427Z","iopub.execute_input":"2025-06-14T18:54:36.190761Z","iopub.status.idle":"2025-06-14T18:55:04.722544Z","shell.execute_reply.started":"2025-06-14T18:54:36.190732Z","shell.execute_reply":"2025-06-14T18:55:04.721765Z"}},"outputs":[{"name":"stdout","text":"calculating bert score\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f86ea46b5d8d48f7a3c32ed680587efa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f846e4cade3e49948a48c3474af3b89f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70d47c54d2334710b33b69934c82a44a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"796a02784e82415db4230e5d9cb6078a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"808dcadfad7e402fafeefae701a604fd"}},"metadata":{}},{"name":"stdout","text":"\nBERTScore Results:\n  Average F1: 0.5487\n  Average Precision: 0.5823\n  Average Recall: 0.5281\n\n--- Sample Inferences for Manual Inspection ---\n\nQuestion: In the instrument formula for a Gingival Margin Trimmer (GMT) used during cavity preparation, what is the second number representing the angle of the cutting edge when access to the distal gingival margin is achieved?\nReference: In the instrument formula for a Gingival Margin Trimmer (GMT) used during cavity preparation, the second number, which represents the angle of the cutting edge, is typically 95 degrees when it is intended for accessing the distal gingival margin. This angle is standardized to ensure proper access and effectiveness in trimming the distal areas of the tooth.\nGenerated: The second number represents the angle of the cutting edge of the Gingival Margin Trimmer.\n\n###Explanation: \nThe formula for the Gingival Margin Trimmer is:\n$$\\frac{0.25}{\\sqrt{3}}\\times\\frac{1}{2}$$\n\nIn the formula, the second number represents the angle of the cutting edge. It is a fraction of the total angle of the cutting edge, which is 2π. This means that the cutting edge is 0.25 radians (or 12.57 degrees) wide.\nBERTScore F1 for this example: 0.6029\n\nQuestion: A 67-year-old man comes to the emergency department complaining of severe abdominal pain for the last several hours. The pain is cramp-like in nature, constant, 8/10, and has worsened over time. It is associated with bilious vomiting. He gives a history of episodic right upper abdominal pain for the past few months, mostly after consuming fatty foods, radiating to the tip of the right scapula. He reports no change in bowel habits, bladder habits, or change in weight. His past medical history includes diabetes and hypertension, and he takes hydrochlorothiazide, metformin, ramipril, and atorvastatin. Temperature is 38.2°C (100.8°F), blood pressure is 110/70 mm Hg, pulse is 102/min, respiratory rate is 20/min, and BMI is 23 kg/m2. On physical examination, his abdomen is distended and diffusely tender.\nLaboratory test\nComplete blood count\nHemoglobin 13 g/dL\nWBC 16,000/mm3\nPlatelets 150,000/mm3\nBasic metabolic panel\nSerum Na+ 148 mEq/L\nSerum K+ 3.3 mEq/L\nSerum Cl- 89 mEq/L\nAn abdominal CT scan is shown. What is the most likely underlying cause of this patient’s current presentation?\nA. Cholelithiasis\nB. Pancreatitis\nC. Peptic ulcer disease\nD. Intestinal adhesion\nE. Ileal stricture\nReference: A. Cholelithiasis\n\nThe presentation and history are consistent with complications arising from gallstone disease, specifically suggesting gallstone ileus. The history of right upper quadrant pain after fatty meals points to gallstones, and the severe abdominal pain with bilious vomiting and abdominal distension are indicative of a bowel obstruction likely due to a gallstone that has migrated and caused a blockage, i.e., gallstone ileus. The elevated white blood cell count and fever suggest an inflammatory process. On the basis of this information, cholelithiasis is the most likely underlying cause of his current presentation.\nGenerated: Question A\n\nThis patient's symptoms are most consistent with acute cholecystitis, a disease that typically causes acute, severe pain with a high bilirubin level and elevated ammonia. A cholecystitis is a complication of gallstone disease and is more common in older patients with chronic liver disease. Inflammation of the gallbladder can lead to gallstones, and the presence of gallstones in the setting of cholecystitis would explain the patient's symptoms. Cholecystitis is characterized by the formation of gallstones and inflammation of the gallbladder.\n\nThe cause of cholecystitis is typically due to inflammation and infection of the gallbladder, which is characterized by elevated bilirubin and ammonia levels. Cholecystitis is also associated with cholangitis, which is inflammation of the biliary tree, and may be associated with elevated liver enzymes.\n\nGallstone disease is more common in older patients with chronic liver disease. The most common cause of cholangitis is gallstone disease, which is more common in older patients. Gallstones are small, round, and calcified, and are more common in patients with gallstone disease. They can be impacted in the gallbladder, which is more common in older patients with chronic liver disease. Patients with gallstone disease are at an increased risk of cholecystitis, which is characterized by inflammation and infection of the gallbladder, as well as gallstones. \n\nCholangitis is inflammation of the biliary tree, which is more common in patients with gallstone disease. Patients with gallstone disease are at an increased risk of cholangitis, which is characterized by inflammation and infection of the biliary tree. Inflammation and infection of the gallbladder are both common in patients with gallstone disease, and these features are typically associated with gallstones. \n\nThe patient's symptoms of abdominal pain, nausea, and vomiting are consistent with acute cholecystitis. The presence of a gallstone is also consistent with acute cholecystitis. The patient's history of gallstone disease is also consistent with acute cholecystitis, as it is more common in patients with chronic liver disease. \n\nThe CT scan is consistent with acute cholecystitis. Gallstones are more common in patients with gallstone disease. Gallstones are calcified, and patients with gallstone disease are at an increased risk of cholangitis, which is characterized by inflammation and infection of the biliary tree. Patients with gallstone disease are also at an\nBERTScore F1 for this example: 0.5751\n\nQuestion: In a diving accident that results in the severing of the spinal cord below the sixth cervical vertebra, which muscle would be affected due to its innervation by spinal levels below this point?\nReference: In a diving accident where the spinal cord is severed below the sixth cervical vertebra (C6), muscles that depend on innervation from spinal levels below this point would be affected. The intrinsic muscles of the hand, such as the interossei, are among those significantly impacted. These muscles are primarily innervated by the C8 and T1 nerves, which are below the injury site at C6. As a result, fine motor functions and intricate movements of the hand and fingers would be compromised.\nGenerated: 6th cervical diaphragm\nThe diaphragm is a flat sheet of muscle that forms the diaphragm and separates the thoracic and abdominal cavities. The diaphragm is innervated by the thoracic spinal cord. In a spinal cord injury, it is severed in the cervical spinal cord, resulting in damage to the diaphragm.\nBERTScore F1 for this example: 0.5647\n\nQuestion: In a study conducted on patients with confirmed HIV infections in Mumbai, where 100 individuals out of 200 were randomly selected to receive a new drug mixed with orange juice, and neither the medical staff nor the patients knew who received the drug, what is the research method used to ensure unbiased results?\nReference: The research method used in this study to ensure unbiased results is called a double-blind, randomized controlled trial. In this type of study, participants are randomly assigned to either the group receiving the treatment or the control group. Additionally, neither the participants nor the researchers know who is receiving the treatment, which helps prevent bias in administering the treatment or reporting results. This approach is considered the gold standard for clinical trials because it minimizes biases and allows for a more accurate evaluation of the drug's effectiveness.\nGenerated: Home\nA. Randomized control trial\nB. Case control study\nC. Case study\nD. Cross-sectional study\nE. Cross over study\nAnswer: A\nBERTScore F1 for this example: 0.4381\n\nQuestion: What is the diagnosis for a homosexual person who feels imposed by a female body and experiences persistent discomfort with their sex?\nReference: The situation you're describing is indicative of gender dysphoria. Gender dysphoria is the distress or discomfort that may occur when a person's gender identity does not align with their sex assigned at birth. In this case, the person is expressing significant discomfort with their female body, which can be a key sign of gender dysphoria. Their sexual orientation, being homosexual, relates to who they are attracted to and doesn't directly impact their feelings about their gender identity. Addressing gender dysphoria often involves exploring one's gender identity with professionals who can provide support and guidance tailored to the individual's experiences and needs.\nGenerated: 1. Female sexual dysfunction\n1. Homosexuality\n2. Sexual confusion\n3. Sexual dysfunction\n4. Sexual attraction to males\n5. Sexual dysfunction\n6. Sexual dysfunction\n7. Sexual dysfunction\n8. Sexual dysfunction\n9. Sexual dysfunction\n10. Sexual dysfunction\n11. Sexual dysfunction\n12. Sexual dysfunction\n13. Sexual dysfunction\n14. Sexual dysfunction\n15. Sexual dysfunction\n16. Sexual dysfunction\n17. Sexual dysfunction\n18. Sexual dysfunction\n19. Sexual dysfunction\n20. Sexual dysfunction\n21. Sexual dysfunction\n22. Sexual dysfunction\n23. Sexual dysfunction\n24. Sexual dysfunction\n25. Sexual dysfunction\n26. Sexual dysfunction\n27. Sexual dysfunction\n28. Sexual dysfunction\n29. Sexual dysfunction\n30. Sexual dysfunction\n31. Sexual dysfunction\n32. Sexual dysfunction\n33. Sexual dysfunction\n34. Sexual dysfunction\n35. Sexual dysfunction\n36. Sexual dysfunction\n37. Sexual dysfunction\n38. Sexual dysfunction\n39. Sexual dysfunction\n40. Sexual dysfunction\n41. Sexual dysfunction\n42. Sexual dysfunction\n43. Sexual dysfunction\n44. Sexual dysfunction\n45. Sexual dysfunction\n46. Sexual dysfunction\n47. Sexual dysfunction\n48. Sexual dysfunction\n49. Sexual dysfunction\n50. Sexual dysfunction\n51. Sexual dysfunction\n52. Sexual dysfunction\n53. Sexual dysfunction\n54. Sexual dysfunction\n55. Sexual dysfunction\n56. Sexual dysfunction\n57. Sexual dysfunction\n58. Sexual dysfunction\n59. Sexual dysfunction\n60. Sexual dysfunction\n61. Sexual dysfunction\n62. Sexual dysfunction\n63. Sexual dysfunction\n64. Sexual dysfunction\n65. Sexual dysfunction\n66. Sexual dysfunction\n67. Sexual dysfunction\n68. Sexual dysfunction\n69. Sexual dysfunction\n70. Sexual dysfunction\n71. Sexual dysfunction\n72. Sexual dysfunction\n73. Sexual dysfunction\n74. Sexual dysfunction\n75. Sexual dysfunction\n76. Sexual dysfunction\n77. Sexual dysfunction\n78. Sexual dysfunction\n79. Sexual dysfunction\n80. Sexual dysfunction\n81. Sexual dysfunction\n82. Sexual dysfunction\n83. Sexual dysfunction\n84. Sexual dysfunction\n85. Sexual dysfunction\n86. Sexual dysfunction\n87. Sexual dysfunction\n88. Sexual dysfunction\n89. Sexual dysfunction\n90. Sexual dysfunction\n91. Sexual dysfunction\n92. Sexual dysfunction\n93. Sexual dysfunction\n94. Sexual dysfunction\n95. Sexual dysfunction\n96. Sexual dysfunction\n97. Sexual dysfunction\n98. Sexual dysfunction\n99. Sexual dysfunction\n100. Sexual dysfunction\nBERTScore F1 for this example: 0.3754\n","output_type":"stream"}],"execution_count":58},{"cell_type":"markdown","source":"## PUSHING TO THE HUGGINGFACE HUB","metadata":{}},{"cell_type":"code","source":"hf_repo_id=\"mokshaik/llama-3-2-1b-medical-qa-lora-finetuned\"\nprint(f\"Uploading model and tokenizer to Hugging Face Hub: {hf_repo_id}...\")\nmodel.push_to_hub(hf_repo_id,commit_message=\"Initial release of Llama 3.2 1B mdeical QA Lora Finetune\")\ntokenizer.push_to_hub(hf_repo_id, commit_message=\"Add tokenizer for Llama 3.2 1B medical QA fine-tune\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T17:39:56.524425Z","iopub.execute_input":"2025-06-14T17:39:56.524732Z","iopub.status.idle":"2025-06-14T17:40:32.737224Z","shell.execute_reply.started":"2025-06-14T17:39:56.524710Z","shell.execute_reply":"2025-06-14T17:40:32.736653Z"}},"outputs":[{"name":"stdout","text":"Uploading model and tokenizer to Hugging Face Hub: mokshaik/llama-3-2-1b-medical-qa-lora-finetuned...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.07G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de671e40754845ceb28a3a0bcefa43a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"896e58b787f54b78aa6d99999afeb47d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c93969095a4a415590ae391f2125aa5b"}},"metadata":{}},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/mokshaik/llama-3-2-1b-medical-qa-lora-finetuned/commit/c9dfe6143cd8a0f57602f5cce41fa83944d34ce1', commit_message='Add tokenizer for Llama 3.2 1B medical QA fine-tune', commit_description='', oid='c9dfe6143cd8a0f57602f5cce41fa83944d34ce1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mokshaik/llama-3-2-1b-medical-qa-lora-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='mokshaik/llama-3-2-1b-medical-qa-lora-finetuned'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":48},{"cell_type":"markdown","source":"# LOADING FROM HUGGING FACE","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import InferenceClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T19:00:27.030470Z","iopub.execute_input":"2025-06-14T19:00:27.031319Z","iopub.status.idle":"2025-06-14T19:00:27.180837Z","shell.execute_reply.started":"2025-06-14T19:00:27.031292Z","shell.execute_reply":"2025-06-14T19:00:27.180051Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nimport torch\n\n# --- 1. Define your model path/ID and load model/tokenizer ---\n# If your 'tuned_model' object is already in memory from previous steps, you don't need to load it again.\n# If you are running this in a *new* script or session, you'll need to load it.\n# Assuming you have saved it locally at './llama-lora-medical'\n# OR uploaded it to Hugging Face Hub at 'mokshaik/llama-3-2-1b-medical-qa-lora-finetuned'\n\n# OPTION A: Load from a local path (if you saved it after merging)\n# model_path = \"./llama-lora-medical\" # Or wherever you saved your merged model\n# tokenizer = AutoTokenizer.from_pretrained(model_path)\n# model = AutoModelForCausalLM.from_pretrained(\n#     model_path,\n#     torch_dtype=torch.bfloat16, # Adjust based on your model's actual dtype\n#     load_in_4bit=True,         # Set to False if you don't want 4-bit quantization\n#     device_map=\"auto\"\n# )\n\n# OPTION B: Load directly from Hugging Face Hub (recommended for clean start in new session)\nmodel_id = \"mokshaik/llama-3-2-1b-medical-qa-lora-finetuned\"\nprint(f\"Loading model and tokenizer from Hugging Face Hub: {model_id}...\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# Ensure consistency with your training (Llama 3 setup)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16, # Crucial to match your training/merging dtype\n    load_in_4bit=True,         # Load in 4-bit for memory efficiency, if compatible\n    device_map=\"auto\"          # Automatically uses GPU (CUDA/MPS) if available\n)\n\nmodel.eval() # Set the model to evaluation mode (important for inference)\nprint(\"Model loaded successfully for local inference!\")\n\n\n# --- 2. Define Generation Configuration ---\n# These parameters control how the model generates text.\n# Ensure they are consistent with what gives you good results.\ngeneration_config = GenerationConfig(\n    max_new_tokens=512,        # Maximum number of tokens to generate\n    do_sample=True,            # Set to True for creative/diverse outputs\n    temperature=0.7,           # Lower for more deterministic, higher for more random\n    top_p=0.9,                 # Nucleus sampling: only consider tokens that sum up to this probability\n    top_k=50,                  # Top-K sampling: only consider top K most likely tokens\n    pad_token_id=tokenizer.pad_token_id, # Essential for batching (though not used in single example)\n    eos_token_id=tokenizer.eos_token_id, # Model stops generating when it outputs this token\n)\n\n# --- 3. Prepare the Prompt ---\n# This is your input question, formatted exactly as your model was trained.\n# As you confirmed, no <s>[INST]...[/INST]</s> wrappers here.\nprompt = \"\"\"### Question:\nGiven the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\n\n### Answer: \"\"\" # Keep the space after 'Answer:' to give the model context to complete\n\n\n# --- 4. Perform Local Inference ---\nprint(\"Performing local inference...\")\ntry:\n    # Tokenize the input prompt and move to the appropriate device (GPU/CPU)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    # Generate the response\n    with torch.no_grad(): # Disable gradient calculations to save memory and speed up\n        generated_output = model.generate(\n            **inputs,\n            generation_config=generation_config,\n        )\n\n    # Decode the generated output\n    # IMPORTANT: Slice the tensor to remove the input prompt's tokens,\n    # so you only get the newly generated answer.\n    response_text = tokenizer.decode(\n        generated_output[0, inputs[\"input_ids\"].shape[1]:], # Slice from after the prompt\n        skip_special_tokens=True # Remove special tokens like <eos>, <pad>\n    ).strip()\n\n    # --- 5. Post-process the Response ---\n    # Clean up any potential artifacts if the model tends to generate parts of the next prompt.\n    if \"### Question:\" in response_text:\n        # If the model starts generating the next question, truncate it.\n        response_text = response_text.split(\"### Question:\")[0].strip()\n\n    print(\"\\nMODEL RESPONSE IS:\")\n    print(response_text)\n\nexcept Exception as e:\n    print(f\"An error occurred during inference: {e}\")\n    # Common issues here:\n    # - Out of memory (OOM) if model is too large for your GPU/RAM.\n    # - Incorrect dtype or quantization settings for loading.\n    # - Model/tokenizer files are corrupted or incomplete.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T19:11:02.665330Z","iopub.execute_input":"2025-06-14T19:11:02.666264Z","iopub.status.idle":"2025-06-14T19:11:26.185023Z","shell.execute_reply.started":"2025-06-14T19:11:02.666230Z","shell.execute_reply":"2025-06-14T19:11:26.184308Z"}},"outputs":[{"name":"stdout","text":"Loading model and tokenizer from Hugging Face Hub: mokshaik/llama-3-2-1b-medical-qa-lora-finetuned...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5b2cc72e4f94a73aa9c028d776c8e8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0d944a82f9a4e26a06cc11bc12083e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bace284433c4bccab3e456f9fbe391e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f27e205b5b429baf2916be2a58b8e1"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.07G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35c7502dd9844acb8ee96eb42c83a202"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caf258ce58234766bf9ae88e85465a12"}},"metadata":{}},{"name":"stderr","text":"`generation_config` default values have been modified to match model-specific defaults: {'bos_token_id': 128000}. If this is not desired, please set these values explicitly.\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully for local inference!\nPerforming local inference...\n\nMODEL RESPONSE IS:\n1\nThe symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg suggest that the patient is experiencing a stroke. A stroke can cause damage to the arteries of the brain, leading to a decrease in blood flow to the brain and potentially resulting in weakness and paralysis. This condition can occur due to a variety of underlying causes, such as atherosclerosis, heart disease, or blood clots. To rule out other potential causes, it is crucial to conduct a thorough medical history and perform a detailed physical examination, including a neurological evaluation. In the context of a recent long-distance trip, the potential underlying cause of the stroke could be due to blood clots formed during the trip. This is why it is important to conduct a thorough medical history and perform a detailed physical examination, including a neurological evaluation.\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}